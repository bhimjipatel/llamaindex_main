{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd032bcb-fefb-48ec-94da-08d49ac26120",
   "metadata": {},
   "source": [
    "# Query Pipeline with Routing\n",
    "\n",
    "Here we showcase our query pipeline with routing.\n",
    "\n",
    "Routing lets us dynamically choose underlying query pipelines to use given the query and a set of choices.\n",
    "\n",
    "We offer this as an out-of-the-box abstraction in our [Router Query Engine](https://docs.llamaindex.ai/en/stable/examples/query_engine/RouterQueryEngine.html) guide. Here we show you how to compose a similar pipeline using our Query Pipeline syntax - this allows you to not only define query engines but easily stitch it into a chain/DAG with other modules across the compute graph.\n",
    "\n",
    "We show this in two ways:\n",
    "1. **Using a Router Component**: This is a Component that is composed on top of other query pipelines, and selects them based on a condition.\n",
    "2. **Using Conditional Edges**: You can make the edges in a graph \"conditional\", meaning that they are only picked if certain conditions are met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531eedc-4f65-457e-8844-55fcc1773154",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load in the Paul Graham essay as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a441905-9007-44d6-b71a-6fc3e5023e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-10 00:31:34--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘pg_essay.txt’\n",
      "\n",
      "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-02-10 00:31:34 (6.78 MB/s) - ‘pg_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533149c-4312-4444-9b45-52afe21731ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caf998-0a88-4c50-b6a4-2a0c412bde5b",
   "metadata": {},
   "source": [
    "## Define Modules\n",
    "\n",
    "We define llm, vector index, summary index, and prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcbdb2-6747-4e65-b1ce-5d40febccb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    ")\n",
    "from typing import Dict, Any, List, Optional\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SummaryIndex,\n",
    ")\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index.schema import NodeWithScore, TextNode\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.selectors import LLMSingleSelector\n",
    "\n",
    "# define HyDE template\n",
    "hyde_str = \"\"\"\\\n",
    "Please write a passage to answer the question: {query_str}\n",
    "\n",
    "Try to include as many key details as possible.\n",
    "\n",
    "Passage: \"\"\"\n",
    "hyde_prompt = PromptTemplate(hyde_str)\n",
    "\n",
    "# define llm\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# define synthesizer\n",
    "summarizer = TreeSummarize(\n",
    "    service_context=ServiceContext.from_defaults(llm=llm)\n",
    ")\n",
    "\n",
    "# define vector retriever\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "vector_query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "# define summary query prompts + retrievers\n",
    "summary_index = SummaryIndex.from_documents(documents)\n",
    "summary_qrewrite_str = \"\"\"\\\n",
    "Here's a question:\n",
    "{query_str}\n",
    "\n",
    "You are responsible for feeding the question to an agent that given context will try to answer the question.\n",
    "The context may or may not be relevant. Rewrite the question to highlight the fact that\n",
    "only some pieces of context (or none) maybe be relevant.\n",
    "\"\"\"\n",
    "summary_qrewrite_prompt = PromptTemplate(summary_qrewrite_str)\n",
    "summary_retriever = summary_index.as_retriever()\n",
    "summary_query_engine = summary_index.as_query_engine()\n",
    "\n",
    "# define selector\n",
    "selector = LLMSingleSelector.from_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d5ff8-ae04-4ea3-bbe0-2c097af71efd",
   "metadata": {},
   "source": [
    "## Setup Query Pipeline with `RouterComponent`\n",
    "\n",
    "In the first approach, we show you how to setup a query pipeline with a `RouterComponent`. The `RouterComponent` specifically takes in one of our `Selector` modules, and given a set of choices (with string descriptions), chooses the relevant choice and calls the relevant sub-component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87a439-88e6-4130-b28f-45268330d3e4",
   "metadata": {},
   "source": [
    "### Construct Query Pipelines\n",
    "\n",
    "Define a query pipeline for vector index, summary index, and join it together with a router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95be2e-517f-4632-a7b8-a2e0dec11d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define summary query pipeline\n",
    "from llama_index.query_pipeline import RouterComponent\n",
    "\n",
    "vector_chain = QueryPipeline(chain=[vector_query_engine])\n",
    "summary_chain = QueryPipeline(\n",
    "    chain=[summary_qrewrite_prompt, llm, summary_query_engine], verbose=True\n",
    ")\n",
    "\n",
    "choices = [\n",
    "    \"This tool answers specific questions about the document (not summary questions across the document)\",\n",
    "    \"This tool answers summary questions about the document (not specific questions)\",\n",
    "]\n",
    "\n",
    "router_c = RouterComponent(\n",
    "    selector=selector,\n",
    "    choices=choices,\n",
    "    components=[vector_chain, summary_chain],\n",
    "    verbose=True,\n",
    ")\n",
    "# top-level pipeline\n",
    "qp = QueryPipeline(chain=[router_c], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda05274-09c5-4b56-b2ba-57f445346e73",
   "metadata": {},
   "source": [
    "### Try out Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36ba65-636f-4fe9-8dee-e318cfe9a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module c0a87442-3165-443d-9709-960e6ddafe7f with input: \n",
      "query: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;5;200mSelecting component 0: The author used a tool to answer specific questions about the document, which suggests that he was engaged in analyzing and extracting specific information from the document during his time in YC..\n",
      "\u001b[0mDuring his time in YC, the author worked on various tasks related to running Y Combinator. This included selecting and helping founders, dealing with disputes between cofounders, figuring out when people were lying, and fighting with people who maltreated the startups. The author also worked on writing essays and internal software for YC.\n"
     ]
    }
   ],
   "source": [
    "# compare with sync method\n",
    "response = qp.run(\"What did the author do during his time in YC?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd595f-31d0-4490-8f18-6132ac240c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module c0a87442-3165-443d-9709-960e6ddafe7f with input: \n",
      "query: What is a summary of this document?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;5;200mSelecting component 1: The summary questions about the document are answered by this tool..\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 0e7e9d49-4c92-45a9-b3bf-0e6ab76b51f9 with input: \n",
      "query_str: What is a summary of this document?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module b0ece4e3-e6cd-4229-8663-b0cd0638683c with input: \n",
      "messages: Here's a question:\n",
      "What is a summary of this document?\n",
      "\n",
      "You are responsible for feeding the question to an agent that given context will try to answer the question.\n",
      "The context may or may not be relev...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module f247ae78-a71c-4347-ba49-d9357ee93636 with input: \n",
      "input: assistant: What is the summary of the document?\n",
      "\n",
      "\u001b[0mThe document discusses the development and evolution of Lisp as a programming language. It highlights how Lisp was originally created as a formal model of computation and later transformed into a programming language with the assistance of Steve Russell. The document also emphasizes the unique power and elegance of Lisp in comparison to other languages.\n"
     ]
    }
   ],
   "source": [
    "response = qp.run(\"What is a summary of this document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a7118-875a-4d4f-996c-fb0bab0c9b55",
   "metadata": {},
   "source": [
    "## Setup Query Pipeline with Conditional Links\n",
    "\n",
    "In the example below we should you how to build our query pipeline with conditional links to route between our summary query engine and router query engine depending on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6e758-1b6f-484f-952e-c89cb62549d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    Link,\n",
    "    FnComponent,\n",
    ")\n",
    "from llama_index.selectors import LLMSingleSelector\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f49f7-624c-4f93-87f0-1474e1b2f999",
   "metadata": {},
   "source": [
    "We first initialize our `LLMSingleSelector` component. Given a set of choices and a user query it will output a selection indicating the choice it picks.\n",
    "\n",
    "Note that the `LLMSingleSelector` can be directly used in a query pipeline. However here we wrap it in a `FnComponent` so that we can return the output as a dictionary of both the selected index and the original user query (this will help when we define our conditional link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d179b914-12b6-4cdd-89b2-9128714135c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\n",
    "    \"This tool answers specific questions about the document (not summary questions across the document)\",\n",
    "    \"This tool answers summary questions about the document (not specific questions)\",\n",
    "]\n",
    "\n",
    "\n",
    "def select_choice(query: str) -> Dict:\n",
    "    selector = LLMSingleSelector.from_defaults()\n",
    "    output = selector.select(choices, query)\n",
    "    return {\"query\": query, \"index\": str(output.ind)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f0386-7dc3-4072-8777-3216613b4a51",
   "metadata": {},
   "source": [
    "We now initialize our Query Pipeline with the modules: input, selector, vector/summary query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb637ac-8aed-4b7c-ab33-57af8dd8c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QueryPipeline(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"selector\": FnComponent(fn=select_choice),\n",
    "        \"vector_retriever\": vector_retriever,\n",
    "        \"summary_retriever\": summary_retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc8ab4-9c25-4e03-a9b0-fd901e9d6679",
   "metadata": {},
   "source": [
    "We now define our links. Input --> selector is standard. What's more interesting here is our conditional link. \n",
    "\n",
    "We input our selector component as the source. \n",
    "\n",
    "We then input a function that produces two outputs, the first being the condition variable and the second being the child component.\n",
    "\n",
    "Lastly, we define a dictionary mapping each condition variable value to the component (which is represented as a dictionary with \"dest\" and \"dest_key\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa6bcf-f42b-4902-ab96-fe50afcae371",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp.add_link(\"input\", \"selector\")\n",
    "qp.add_link(\n",
    "    \"selector\",\n",
    "    \"vector_retriever\",\n",
    "    condition_fn=lambda x: x[\"index\"] == \"0\",\n",
    "    input_fn=lambda x: x[\"query\"],\n",
    ")\n",
    "qp.add_link(\n",
    "    \"selector\",\n",
    "    \"summary_retriever\",\n",
    "    condition_fn=lambda x: x[\"index\"] == \"1\",\n",
    "    input_fn=lambda x: x[\"query\"],\n",
    ")\n",
    "qp.add_link(\"vector_retriever\", \"summarizer\", dest_key=\"nodes\")\n",
    "qp.add_link(\"summary_retriever\", \"summarizer\", dest_key=\"nodes\")\n",
    "qp.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2305e7-3a4b-4e3d-84a5-f5469f096f55",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "The benefit of conditional links is that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63381dfa-4338-47d5-b0d3-12964391f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_dag.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"rag_dag.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2993a3ca0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(qp.clean_dag)\n",
    "net.show(\"rag_dag.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d8696-0579-4b1d-be97-e95a26f26911",
   "metadata": {},
   "source": [
    "### Try out Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c47e77-137e-4913-914f-b80f3ccfd30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module selector with input: \n",
      "query: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module vector_retriever with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summarizer with input: \n",
      "query_str: What did the author do during his time in YC?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='e7937e10-61ca-435a-a6a7-4232cdd7bed8', embedding=None, metadata={'file_path': 'pg_essay.txt', 'file_name': 'pg_essay.txt', 'file_type': 'text/plain', 'file_size': 750...\n",
      "\n",
      "\u001b[0mDuring his time in YC, the author worked on various tasks related to running the program. He selected and helped founders, dealt with disputes between cofounders, figured out when people were lying, and fought with people who maltreated the startups. Additionally, he wrote all of YC's internal software and worked on other projects such as writing essays and managing Hacker News. The author worked hard and wanted YC to be successful, so he dedicated a significant amount of time and effort to his responsibilities.\n"
     ]
    }
   ],
   "source": [
    "# compare with sync method\n",
    "response = qp.run(input=\"What did the author do during his time in YC?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05f4c1-aaad-44ff-8b67-3e7a29afea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f9cbc-9bbe-40af-a06e-b49e7a7b80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "input: What is a summary of this document?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module selector with input: \n",
      "query: What is a summary of this document?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summary_retriever with input: \n",
      "input: What is a summary of this document?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summarizer with input: \n",
      "query_str: What is a summary of this document?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='f6d0a4c7-3806-4169-8e87-ba7870335312', embedding=None, metadata={'file_path': 'pg_essay.txt', 'file_name': 'pg_essay.txt', 'file_type': 'text/plain', 'file_size': 750...\n",
      "\n",
      "\u001b[0mThe document provides a personal narrative of the author's journey from studying art in Florence to working at a software company in the US. It discusses their experiences at the Accademia di Belli Arti in Florence, their time working at Interleaf, their decision to drop out of art school and pursue painting and writing books on Lisp, and their failed attempt to start an online art gallery business. The document also covers the founding and early years of Viaweb, a company that aimed to build online stores, and the author's experiences after selling the company, including their struggles with painting and finding a sense of purpose. It concludes with the author's decision to start Y Combinator, an investment firm, and their exploration of Lisp programming language and the development of a new Lisp language called Bel.\n"
     ]
    }
   ],
   "source": [
    "response = qp.run(input=\"What is a summary of this document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e935-e390-41fe-abaf-f21c1282b83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.source_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
